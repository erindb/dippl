---
layout: update
title: Explanations Update 2016.07.11
description: Sources of uncertainty in explanations
---

## Motivation

"B 'cause A" is an utterance. We should be able to articulate a semantics for it. Our hypothesis is that it is a function on a particular world state:

Given the actual state of the world, counterfactually if A had been false, then B would have been false.

We have a model of counterfactual reasoning that can determine the truth value of this utterance for a particular world.

But in order to understand a speaker's choice for one explanation over another, we need to know what QUDs the speaker might be addressing. We know that "B 'cause A" means something more than "B and A", so there must be other variables beyond A and B that are being communicated.

What are these other variables?

## Sources of uncertainty

### Observable states of variables

As stated above, there must be other QUDs besides the current states of A and B, or else "B 'cause A" would have no further meaning than "B and A". What other observable states could the speaker be communicating?

1. The values of other variables (C, D, E, F, G, ...)
2. Future values of A and B

It could be that explanations update the posterior distributions of a rich set of unmentioned variables.

<!-- ~~~
~~~ -->

It could be that explanations allow a listener to infer the states of mentioned variables in the future, under different circumstances and with partial knowledge.

<!-- ~~~
~~~ -->

### Causal structure of the world

One intuitive QUD for explanations is the causal structure of the world. This includes:

* The existence of a causal connection between each pair of variables
* The direction of the causal connections
* Strengths of the causal connections
* Background probabilities of all variables

The parameters of the causal structure of the world are not independent of one another, so when we counterfactualize, we should probably sample a model at uniform from the space of all possible parameters.

But here's a problem: when we evaluate the truth of a counterfactual statement, the causal relationships need to stay in place, at least to some extent. Otherwise they couldn't communicate much about those causal relationships. Or could they...?

#### Communicating causal models

Here's an example where we *can* communicate something about the causal structure:

~~~
// are A and B causally connected?

// if A and B are causally connected,
// then B is true whenever A is.
// otherwise, B is false.
var latentsModel = function() {
	return {
		model: uniformDraw(["A->B", "A,B"]),
		A: flip()
	};
};

var statesModel = function(latents) {
	var A = latents.A;
	var B = latents.model=="A->B" ? A : false;
	var connected = latents.model=="A->B";
	return { A: A, B: B, connected: connected };
};

///fold:
	var infer = function(fn) {return Infer({method: 'enumerate'}, fn); };
	var counterfactualizeLatents = function(model, actual) {
		var s = 0.53; //stickiness
		var totallyDifferent = model();

		return mapObject(function(key, value) {
			return flip(s) ? actual[key] : totallyDifferent[key];
		}, actual);
	};

	var counterfactually = function(args) {
		var ifVariable = args.ifVariable;
		var hadBeen = args.hadBeen;
		var thenVariable = args.thenVariable;
		var wouldhaveBeen = args.wouldhaveBeen;
		var givenActualLatents = args.givenActualLatents;
		var givenActualStates = args.givenActualStates ? args.givenActualStates : statesModel(givenActualLatents);
		return infer(function() {
			var cfLatents = counterfactualizeLatents(latentsModel, givenActualLatents);
			var cfStates = statesModel(cfLatents);
			condition(cfStates[ifVariable]==hadBeen);
			return cfStates[thenVariable]==wouldhaveBeen
		});
	};

	var latents = latentsModel();
	var states = statesModel(latents);
	display(states);

	var ifNotAThenNotB = counterfactually({
		ifVariable: "A",
		hadBeen: false,
		thenVariable: "B",
		wouldhaveBeen: false,
		givenActualLatents: {model: "A->B", A: true}
	});
	var ifNotBThenNotA = counterfactually({
		ifVariable: "B",
		hadBeen: false,
		thenVariable: "A",
		wouldhaveBeen: false,
		givenActualLatents: {model: "A->B", A: true}
	});
///

print("If not A, then not B");
viz.bar(ifNotAThenNotB, {horizontal: true});

print("If not B, then not A");
viz.auto(ifNotBThenNotA, {horizontal: true});
~~~

Here either B is true whenever A is true, or B is false. In this situation, "A because B" is more likely to be true of the actual world where both are true than "B because A" is.



<!-- Counterfactualization when causal structure is held constant lets us see the causal relationships between variables in a way that statistical correlation ... -->