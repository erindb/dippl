---
layout: update
title: Explanations Update 2016.07.11
description: Sources of uncertainty in explanations
---

## Motivation

"B 'cause A" is an utterance. We should be able to articulate a semantics for it. Our hypothesis is that it is a function on a particular world state:

Given the actual state of the world, counterfactually if A had been false, then B would have been false.

We have a model of counterfactual reasoning that can determine the truth value of this utterance for a particular world.

But in order to understand a speaker's choice for one explanation over another, we need to know what QUDs the speaker might be addressing. We know that "B 'cause A" means something more than "B and A", so there must be other variables beyond A and B that are being communicated.

What are these other variables?

## Sources of uncertainty

### Observable states of variables

As stated above, there must be other QUDs besides the current states of A and B, or else "B 'cause A" would have no further meaning than "B and A". What other observable states could the speaker be communicating?

1. The values of other variables (C, D, E, F, G, ...)
2. Future values of A and B

It could be that explanations update the posterior distributions of a rich set of unmentioned variables.

<!-- ~~~
~~~ -->

It could be that explanations allow a listener to infer the states of mentioned variables in the future, under different circumstances and with partial knowledge.

<!-- ~~~
~~~ -->

### Causal structure of the world

One intuitive QUD for explanations is the causal structure of the world. This includes:

* The existence of a causal connection between each pair of variables
* The direction of the causal connections
* Strengths of the causal connections
* Background probabilities of all variables

The parameters of the causal structure of the world are not independent of one another, so when we counterfactualize, we should probably sample a model at uniform from the space of all possible parameters.

But here's a problem: when we evaluate the truth of a counterfactual statement, the causal relationships need to stay in place, at least to some extent. Otherwise they couldn't communicate much about those causal relationships. Or could they...?

#### Communicating causal models

Let's say there are two variables, A and B. They are either causally connected or they are not. If they are causally connected, then B is deterministically true whenever A is. Otherwise, B is always false.

~~~
// are A and B causally connected?

// if A and B are causally connected,
// then B is true whenever A is.
// otherwise, B is false.
var latentsModel = function() {
	return {
		model: uniformDraw(["A->B", "A,B"]),
		A: flip()
	};
};

var statesModel = function(latents) {
	var A = latents.A;
	var B = latents.model=="A->B" ? A : false;
	var connected = latents.model=="A->B";
	return { A: A, B: B, connected: connected };
};

///fold:
	var infer = function(fn) {return Infer({method: 'enumerate'}, fn); };
	var counterfactualizeLatents = function(model, actual) {
		var s = 0.53; //stickiness
		var totallyDifferent = model();

		return mapObject(function(key, value) {
			return flip(s) ? actual[key] : totallyDifferent[key];
		}, actual);
	};

	var counterfactually = function(args) {
		var ifVariable = args.ifVariable;
		var hadBeen = args.hadBeen;
		var thenVariable = args.thenVariable;
		var wouldhaveBeen = args.wouldhaveBeen;
		var givenActualLatents = args.givenActualLatents;
		var givenActualStates = args.givenActualStates ? args.givenActualStates : statesModel(givenActualLatents);
		return infer(function() {
			var cfLatents = counterfactualizeLatents(latentsModel, givenActualLatents);
			var cfStates = statesModel(cfLatents);
			condition(cfStates[ifVariable]==hadBeen);
			return cfStates[thenVariable]==wouldhaveBeen
		});
	};

	var probTrue = function(dist) {
		return Math.exp(dist.score(true));
	};

	print("How likely is 'If not A, then not B' to be true in a world where...");

	print("both are true and connected: " + probTrue(counterfactually({
		ifVariable: "A",
		hadBeen: false,
		thenVariable: "B",
		wouldhaveBeen: false,
		givenActualLatents: {model: "A->B", A: true}
	})));

	print("both are true and NOT connected: " + probTrue(counterfactually({
		ifVariable: "A",
		hadBeen: false,
		thenVariable: "B",
		wouldhaveBeen: false,
		givenActualLatents: {model: "A,B", A: true}
	})));

	print("How likely is 'If not B, then not A' to be true in a world where...");

	print("both are true and connected: " + probTrue(counterfactually({
		ifVariable: "B",
		hadBeen: false,
		thenVariable: "A",
		wouldhaveBeen: false,
		givenActualLatents: {model: "A->B", A: true}
	})));

	print("both are true and NOT connected: " + probTrue(counterfactually({
		ifVariable: "B",
		hadBeen: false,
		thenVariable: "A",
		wouldhaveBeen: false,
		givenActualLatents: {model: "A,B", A: true}
	})));
///
~~~

<!--
Here either B is true whenever A is true, or B is false. In this situation, "A because B" is more likely to be true of the actual world where both are true than "B because A" is.

This asymmetry remains when B has a low-valued background probability of occurring regardless of A's value.

~~~
// add a background probability for B.
// are A and B causally connected?

// if A and B are causally connected,
// then B is true whenever A is.
// otherwise, B is false.
var latentsModel = function() {
	return {
		model: uniformDraw(["A->B", "A,B"]),
		A: flip(),
		Bbackground: flip(0.1)
	};
};

var statesModel = function(latents) {
	var A = latents.A;
	var B = (latents.model=="A->B") ? (A || latents.Bbackground ? true : false) : (latents.Bbackground ? true : false);
	var connected = latents.model=="A->B";
	return { A: A, B: B, connected: connected };
};

///fold:
	var infer = function(fn) {return Infer({method: 'enumerate'}, fn); };

	var counterfactualizeLatents = function(model, actual) {
		var s = 0.53; //stickiness
		var totallyDifferent = model();

		return mapObject(function(key, value) {
			return flip(s) ? actual[key] : totallyDifferent[key];
		}, actual);
	};

	var counterfactually = function(args) {
		var ifVariable = args.ifVariable;
		var hadBeen = args.hadBeen;
		var thenVariable = args.thenVariable;
		var wouldhaveBeen = args.wouldhaveBeen;
		return infer(function() {

			var actulLatents = args.givenActualLatents ? args.givenActualLatents : latentsModel();
			var actualStates = statesModel(actulLatents);
			if (args.givenActualStates) {
				condition(_.isEqual(args.givenActualStates, actualStates));
			}

			var cfLatents = counterfactualizeLatents(latentsModel, actulLatents);
			var cfStates = statesModel(cfLatents);
			condition(cfStates[ifVariable]==hadBeen);
			return cfStates[thenVariable]==wouldhaveBeen
		});
	};

	print("If not A then not B (when connected)");
	viz.auto(counterfactually({
		ifVariable: "A",
		hadBeen: false,
		thenVariable: "B",
		wouldhaveBeen: false,
		givenActualStates: {connected: true, A: true, B: true}
	}));
	print("If not A then not B (when not connected)");
	viz.auto(counterfactually({
		ifVariable: "A",
		hadBeen: false,
		thenVariable: "B",
		wouldhaveBeen: false,
		givenActualStates: {connected: false, A: true, B: true}
	}));
	print("If not B then not A (when connected)")
	viz.auto(counterfactually({
		ifVariable: "B",
		hadBeen: false,
		thenVariable: "A",
		wouldhaveBeen: false,
		givenActualStates: {connected: true, A: true, B: true}
	}));
	print("If not B then not A (when not connected)")
	viz.auto(counterfactually({
		ifVariable: "B",
		hadBeen: false,
		thenVariable: "A",
		wouldhaveBeen: false,
		givenActualStates: {connected: false, A: true, B: true}
	}));
///
~~~

When we increase the background probability for B, "if not A then not B" becomes mostly false, because the probability of B being false across all counterfactual worlds goes down. However, the statement is *more appropriate* when there is a causal connection between the variables. As a result, "B 'cause A" can communicate a causal connection even when the background probability of B is quite high.

~~~
"i need to prove that statement."
~~~ -->

<!-- Counterfactualization when causal structure is held constant lets us see the causal relationships between variables in a way that statistical correlation ... -->